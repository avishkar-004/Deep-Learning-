{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network** Implementation from Scratch using **Iris Dataset**"
      ],
      "metadata": {
        "id": "azJlsd9_6MRT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np7I0B4546Bz",
        "outputId": "ea950ace-c515-4f44-d95a-7b67e3b787f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000, Loss: 1.0985\n",
            "Epoch 100/1000, Loss: 0.5751\n",
            "Epoch 200/1000, Loss: 0.2933\n",
            "Epoch 300/1000, Loss: 0.1786\n",
            "Epoch 400/1000, Loss: 0.1196\n",
            "Epoch 500/1000, Loss: 0.0927\n",
            "Epoch 600/1000, Loss: 0.0788\n",
            "Epoch 700/1000, Loss: 0.0707\n",
            "Epoch 800/1000, Loss: 0.0654\n",
            "Epoch 900/1000, Loss: 0.0618\n",
            "\n",
            "Train Accuracy: 98.33%\n",
            "\n",
            "Test Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_one_hot = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Neural Network Parameters\n",
        "input_size = X_train.shape[1]  # 4 features\n",
        "hidden_size = 10  # 10 neurons in the hidden layer\n",
        "output_size = y_train.shape[1]  # 3 output classes\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "\n",
        "W1 = np.random.randn(input_size, hidden_size) * 0.01  # Weight for input to hidden layer\n",
        "b1 = np.zeros((1, hidden_size))  # Bias for hidden layer\n",
        "\n",
        "W2 = np.random.randn(hidden_size, output_size) * 0.01  # Weight for hidden to output layer\n",
        "b2 = np.zeros((1, output_size))  # Bias for output layer\n",
        "\n",
        "# Activation functions and their derivatives\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # For numerical stability\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_pred, y_true):\n",
        "    m = y_true.shape[0]\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-8)) / m  # Add small epsilon for numerical stability\n",
        "\n",
        "# Forward Pass\n",
        "def forward(X):\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = softmax(z2)\n",
        "    return a1, a2\n",
        "\n",
        "# Backward Pass (Gradient Descent and Backpropagation)\n",
        "def backward(X, y, a1, a2):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # Output layer error\n",
        "    dz2 = a2 - y\n",
        "    dW2 = np.dot(a1.T, dz2) / m\n",
        "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Hidden layer error\n",
        "    dz1 = np.dot(dz2, W2.T) * relu_derivative(a1)\n",
        "    dW1 = np.dot(X.T, dz1) / m\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "# Training the model using gradient descent\n",
        "def train(X_train, y_train, learning_rate=0.01, epochs=1000):\n",
        "    global W1, b1, W2, b2\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        a1, a2 = forward(X_train)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = cross_entropy_loss(a2, y_train)\n",
        "\n",
        "        # Backward pass\n",
        "        dW1, db1, dW2, db2 = backward(X_train, y_train, a1, a2)\n",
        "\n",
        "        # Update the weights and biases using gradient descent\n",
        "        W1 -= learning_rate * dW1\n",
        "        b1 -= learning_rate * db1\n",
        "        W2 -= learning_rate * dW2\n",
        "        b2 -= learning_rate * db2\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluating the model\n",
        "def predict(X):\n",
        "    _, a2 = forward(X)\n",
        "    return np.argmax(a2, axis=1)\n",
        "\n",
        "# Training the network\n",
        "train(X_train, y_train, learning_rate=0.1, epochs=1000)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_train = predict(X_train)\n",
        "y_pred_test = predict(X_test)\n",
        "\n",
        "y_true_train = np.argmax(y_train, axis=1)\n",
        "y_true_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "train_accuracy = np.mean(y_pred_train == y_true_train) * 100\n",
        "test_accuracy = np.mean(y_pred_test == y_true_test) * 100\n",
        "\n",
        "print(f\"\\nTrain Accuracy: {train_accuracy:.2f}%\")\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network** Implementation from Scratch using **MINST Dataset**"
      ],
      "metadata": {
        "id": "u8M3EGjK6LUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X = mnist.data\n",
        "\n",
        "# Convert target labels to NumPy array and cast to integers\n",
        "y = np.array(mnist.target).astype(int)\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse_output=False)  # Change sparse=False to sparse_output=False for compatibility\n",
        "y_one_hot = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Neural Network Parameters\n",
        "input_size = X_train.shape[1]  # 784 features (28x28 pixels)\n",
        "hidden_size = 128  # 128 neurons in the hidden layer\n",
        "output_size = y_train.shape[1]  # 10 output classes (digits 0-9)\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "\n",
        "W1 = np.random.randn(input_size, hidden_size) * 0.01  # Weight for input to hidden layer\n",
        "b1 = np.zeros((1, hidden_size))  # Bias for hidden layer\n",
        "\n",
        "W2 = np.random.randn(hidden_size, output_size) * 0.01  # Weight for hidden to output layer\n",
        "b2 = np.zeros((1, output_size))  # Bias for output layer\n",
        "\n",
        "# Activation functions and their derivatives\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # For numerical stability\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_pred, y_true):\n",
        "    m = y_true.shape[0]\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-8)) / m  # Add small epsilon for numerical stability\n",
        "\n",
        "# Forward Pass\n",
        "def forward(X):\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = softmax(z2)\n",
        "    return a1, a2\n",
        "\n",
        "# Backward Pass (Gradient Descent and Backpropagation)\n",
        "def backward(X, y, a1, a2):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # Output layer error\n",
        "    dz2 = a2 - y\n",
        "    dW2 = np.dot(a1.T, dz2) / m\n",
        "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Hidden layer error\n",
        "    dz1 = np.dot(dz2, W2.T) * relu_derivative(a1)\n",
        "    dW1 = np.dot(X.T, dz1) / m\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "# Training the model using gradient descent\n",
        "def train(X_train, y_train, learning_rate=0.01, epochs=1000):\n",
        "    global W1, b1, W2, b2\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        a1, a2 = forward(X_train)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = cross_entropy_loss(a2, y_train)\n",
        "\n",
        "        # Backward pass\n",
        "        dW1, db1, dW2, db2 = backward(X_train, y_train, a1, a2)\n",
        "\n",
        "        # Update the weights and biases using gradient descent\n",
        "        W1 -= learning_rate * dW1\n",
        "        b1 -= learning_rate * db1\n",
        "        W2 -= learning_rate * dW2\n",
        "        b2 -= learning_rate * db2\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluating the model\n",
        "def predict(X):\n",
        "    _, a2 = forward(X)\n",
        "    return np.argmax(a2, axis=1)\n",
        "\n",
        "# Training the network\n",
        "train(X_train, y_train, learning_rate=0.1, epochs=1000)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_train = predict(X_train)\n",
        "y_pred_test = predict(X_test)\n",
        "\n",
        "y_true_train = np.argmax(y_train, axis=1)\n",
        "y_true_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "train_accuracy = np.mean(y_pred_train == y_true_train) * 100\n",
        "test_accuracy = np.mean(y_pred_test == y_true_test) * 100\n",
        "\n",
        "print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqqVfMP26qNK",
        "outputId": "dffff247-1c70-45cb-890f-a29539a2b132"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000, Loss: 2.3040\n",
            "Epoch 100/1000, Loss: 0.4044\n",
            "Epoch 200/1000, Loss: 0.2836\n",
            "Epoch 300/1000, Loss: 0.2402\n",
            "Epoch 400/1000, Loss: 0.2124\n",
            "Epoch 500/1000, Loss: 0.1914\n",
            "Epoch 600/1000, Loss: 0.1746\n",
            "Epoch 700/1000, Loss: 0.1608\n",
            "Epoch 800/1000, Loss: 0.1491\n",
            "Epoch 900/1000, Loss: 0.1389\n",
            "Train Accuracy: 96.29%\n",
            "Test Accuracy: 95.24%\n"
          ]
        }
      ]
    }
  ]
}